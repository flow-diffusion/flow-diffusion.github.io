<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Learning to Act from Actionless Videos through Dense Correspondences">

    <title>Learning to Act from Actionless Videos through Dense Correspondences</title>
    <!-- Bootstrap core CSS -->
    <!--link href="bootstrap.min.css" rel="stylesheet"-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
    <!--    <link rel="icon" href="img/favicon.gif" type="image/gif">-->
</head>

<body>
    <div class="jumbotron jumbotron-fluid">
        <div class="container"></div>
        <h2>Learning to Act from Actionless Video<br> through Dense Correspondences</h2>
        <p><br /></p>
        <div class="authors">
            <div>Po-Chen Ko<sup>1</sup></div>
            <div>Jiayuan Mao<sup>2</sup></div>
            <div>Yilun Du<sup>2</sup></div>
            <div>Shao-Hua Sun<sup>1</sup></div>
            <div>Josh Tenenbaum<sup>2</sup></div>
        </div>
        <div class="faculty">
            <div><sup>1</sup> National Taiwan University</div>
            <div><sup>2</sup> MIT</div>
        </div>
    </div>

    <div class="container">
        <div class="section">
            <div class="row-align-items-center">
                <div class="col justify-content-center text-center">
                    <div class="overlay">
                        <p>
                            <a href="flow_diffusion_iclr2024.pdf">paper</a>
                        </p>
                    </div>
                </div>
            </div>
            <p>
                In this work, we present an approach to construct a video-based robot policy capable of successfully executing diverse tasks across different robots and environments without the need of any action annotations. Our method leverages images as a task-agnostic
                representation, encoding both the state and action information. By synthesizing videos that "hallucinate" robot executing actions and in combination with dense correspondences between frames, our approach can infer the closed-formed action
                to execute to an environment without the need of any explicit action labels. This unique capability allows us to train the policy solely based on RGB videos and deploy learned policies to various robotic tasks. We demonstrate the efficacy
                of our approach in learning policies on table-top manipulation and navigation tasks. Additionally, we contribute an open-source framework for efficient video modeling, enabling the training of high-fidelity policy models using just 4 GPUs
                within a single day.
            </p>

            <p><br /></p>
        </div>

        <div class="section">
            <h2>Table of contents</h2>
            <hr>
            <ul>
                <li><a href="#framework">Framework Overview</a></li>
                <li><a href="#mwresults">Meta-World Results</a></li>
                <li><a href="#ithorresults">iTHOR Results</a></li>
                <li><a href="#realresults">Real-World Results</a></li>
                <li><a href="#oodresults">Learning from Out Of Domain data</a></li>
                <li><a href="#vidgencomparison">Comparison of first frame conditioning strategy and different text encoders</a></li>
                <li><a href="#bridge0shot">Zero shot generalization on real-world scene of Bridge model</a></li>
                <li><a href="#vidgenresults">Video generation results</a></li>
                <li><a href="#DDIMresults">DDIM results</a></li>
            </ul>
            <p><br /></p>
        </div>


        <div class="section">
            <a name="framework"></a>
            <h2>Framework Overview</h2>
            <hr>
            <ul>
                <li>(a) Our model takes the RGBD observation of the current environmental state and a textual goal description as its input.</li>
                <li>(b) It first synthesizes a video of imagined execution of the task using a diffusion model.</li>
                <li>(c) Next, it estimates the optical flow between adjacent frames in the video.</li>
                <li>(d) Finally, it leverages the optical flow as dense correspondences between frames and the depth of the first frame to compute SE(3) transformations of the target object, and subsequently, robot arm commands.</li>
            </ul>
            <p>
                For more details, please refer to our <a href="flow_diffusion_iclr2024.pdf">paper</a>.
            </p>
            <div class="row align-items-center">
                <img src="img/framework.png" width="100%">
            </div>
            <p><br /></p>
        </div>

        <div class="section">
            <a name="mwresults"></a>
            <h2>Meta-World Results</h2>
            <hr>
            <p>
                Below, we provide some examples of executions of our Meta-World experiments.
            </p>
            <div class="row align-items-center">
                <div class="col justify-content-center text-center">
                    <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                        <source src="img/MW/assembly/executed_video.mp4" type="video/mp4">
                    </video>
                    <div class="overlay">
                        <div class="text"><b>Task: </b>Assembly</div>
                    </div>
                </div>
                <div class="col justify-content-center text-center">
                    <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                        <source src="img/MW/door-open/executed_video.mp4" type="video/mp4">
                    </video>
                    <div class="overlay">
                        <div class="text"><b>Task: </b>Door Open</div>
                    </div>
                </div>
            </div>
            <p><br /></p>
            <div class="row align-items-center">
                <div class="col justify-content-center text-center">
                    <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                        <source src="img/MW/hammer/executed_video.mp4" type="video/mp4">
                    </video>
                    <div class="overlay">
                        <div class="text"><b>Task: </b>Hammer</div>
                    </div>
                </div>
                <div class="col justify-content-center text-center">
                    <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                        <source src="img/MW/shelf-place/executed_video.mp4" type="video/mp4">
                    </video>
                    <div class="overlay">
                        <div class="text"><b>Task: </b>Shelf Place</div>
                    </div>
                </div>
            </div>
            <p><br /></p>
        </div>

        <div class="section">
            <a name="ithorresults"></a>
            <h2>iTHOR Results</h2>
            <hr>
            <p>
                Below, we provide some examples of executions of our iTHOR experiments.
            </p>
            <div class="row align-items-center">
                <div class="col justify-content-center text-center">
                    <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                        <source src="img/iTHOR/Pillow/execution_results/video.mp4" type="video/mp4">
                    </video>
                    <div class="overlay">
                        <div class="text"><b>Task: </b>Pillow</div>
                    </div>
                </div>
                <div class="col justify-content-center text-center">
                    <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                        <source src="img/iTHOR/SoapBar/execution_results/video.mp4" type="video/mp4">
                    </video>
                    <div class="overlay">
                        <div class="text"><b>Task: </b>Soap Bar</div>
                    </div>
                </div>
            </div>
            <p><br /></p>
            <div class="row align-items-center">
                <div class="col justify-content-center text-center">
                    <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                        <source src="img/iTHOR/Television/execution_results/video.mp4" type="video/mp4">
                    </video>
                    <div class="overlay">
                        <div class="text"><b>Task: </b>Television</div>
                    </div>
                </div>
                <div class="col justify-content-center text-center">
                    <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                        <source src="img/iTHOR/Toaster/execution_results/video.mp4" type="video/mp4">
                    </video>
                    <div class="overlay">
                        <div class="text"><b>Task: </b>Toaster</div>
                    </div>
                </div>
            </div>
            <p><br /></p>
        </div>

        <div class="section">
            <a name="realresults"></a>
            <h2>Real-World Results</h2>
            <hr>
            <p>
                Below, we provide some examples of executions of our real-world experiments.
            </p>
            <div class="row align-items-center">
                <div class="col justify-content-center text-center">
                    <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                        <source src="img/Real/put_apple_in_plate.mp4" type="video/mp4">
                    </video>
                    <div class="overlay">
                        <div class="text"><b>Task: </b>put apple in plate</div>
                    </div>
                </div>
            </div>
            <p><br /></p>
            <div class="row align-items-center">
                <div class="col justify-content-center text-center">
                    <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                        <source src="img/Real/put_banana_in_plate.mp4" type="video/mp4">
                    </video>
                    <div class="overlay">
                        <div class="text"><b>Task: </b>put banana in plate</div>
                    </div>
                </div>
            </div>
            <p><br /></p>
            <div class="row align-items-center">
                <div class="col justify-content-center text-center">
                    <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                        <source src="img/Real/put_peach_in_bowl.mp4" type="video/mp4">
                    </video>
                    <div class="overlay">
                        <div class="text"><b>Task: </b>put peach in blue bowl</div>
                    </div>
                </div>
            </div>
            <p><br /></p>
        </div>

        <div class="section">
            <a name="oodresults"></a>
            <h2>Learning from Out Of Domain data</h2>
            <hr>
            <p>
                Below, we provide some examples of executions of our OOD experiments.
            </p>
            <p>
                <b>Successful executions</b>:
            </p>
            <div class="row align-items-center">
                <div class="col justify-content-center text-center">
                    <img src="img/OOD/success/input.png" width="100%">
                    <div class="overlay">
                        <div class="text">input image</div>
                    </div>
                </div>
                <div class="col justify-content-center text-center">
                    <img src="img/OOD/success/plan.gif" width="100%">
                    <div class="overlay">
                        <div class="text">generated video plan</div>
                    </div>
                </div>
                <div class="col justify-content-center text-center">
                    <img src="img/OOD/success/execution.gif" width="100%">
                    <div class="overlay">
                        <div class="text">execution</div>
                    </div>
                </div>
            </div>
            <p><br /></p>
            <div class="row align-items-center">
                <div class="col justify-content-center text-center">
                    <img src="img/OOD/success2/input.png" width="100%">
                    <div class="overlay">
                        <div class="text">input image</div>
                    </div>
                </div>
                <div class="col justify-content-center text-center">
                    <img src="img/OOD/success2/plan.gif" width="100%">
                    <div class="overlay">
                        <div class="text">generated video plan</div>
                    </div>
                </div>
                <div class="col justify-content-center text-center">
                    <img src="img/OOD/success2/execution.gif" width="100%">
                    <div class="overlay">
                        <div class="text">execution</div>
                    </div>
                </div>
            </div>
            <p><br /></p>
            <p>
                <b>Failed executions</b>:
            </p>
            <div class="row align-items-center">
                <div class="col justify-content-center text-center">
                    <img src="img/OOD/failed/input.png" width="100%">
                    <div class="overlay">
                        <div class="text">input image</div>
                    </div>
                </div>
                <div class="col justify-content-center text-center">
                    <img src="img/OOD/failed/plan.gif" width="100%">
                    <div class="overlay">
                        <div class="text">generated video plan</div>
                    </div>
                </div>
                <div class="col justify-content-center text-center">
                    <img src="img/OOD/failed/execution.gif" width="100%">
                    <div class="overlay">
                        <div class="text">execution</div>
                    </div>
                </div>
            </div>
            <p><br /></p>
            <div class="row align-items-center">
                <div class="col justify-content-center text-center">
                    <img src="img/OOD/failed2/input.png" width="100%">
                    <div class="overlay">
                        <div class="text">input image</div>
                    </div>
                </div>
                <div class="col justify-content-center text-center">
                    <img src="img/OOD/failed2/plan.gif" width="100%">
                    <div class="overlay">
                        <div class="text">generated video plan</div>
                    </div>
                </div>
                <div class="col justify-content-center text-center">
                    <img src="img/OOD/failed2/execution.gif" width="100%">
                    <div class="overlay">
                        <div class="text">execution</div>
                    </div>
                </div>
            </div>
            <p><br /></p>
        </div>

        <div class="section">
            <a name="vidgencomparison"></a>
            <h2>Comparison of first frame conditioning strategy and different text encoders</h2>
            <hr>
            <p>
                In this section, we provide a line chart to compare the performance of different text encoders and first frame conditioning strategy. cat_c: first frame is concatenated with noisy video in RGB dimension (Ours). cat_t: first frame is concatenated with
                noisy video in time dimension. CLIP: CLIP text encoder (63M). T5: T5 base encoder (110M).
            </p>
            <div class="row align-items-center">
                <img src="img/mse_plot_final.png" width="100%">
            </div>
            <p>
                Below we provide some qualitative examples of different first frame conditioning strategy vs GT video. The results were generated from bridge model during mid-early stage of training (40k steps).
            </p>
            <div class="row align-items-center">
                <img src="img/cat_strategy_qualitative/1.png" width="100%">
            </div>
            <p><br /></p>
            <div class="row align-items-center">
                <img src="img/cat_strategy_qualitative/2.png" width="100%">
            </div>
            <p><br /></p>
            <div class="row align-items-center">
                <img src="img/cat_strategy_qualitative/3.png" width="100%">
            </div>
            <p><br /></p>
            <div class="row align-items-center">
                <img src="img/cat_strategy_qualitative/4.png" width="100%">
            </div>
            <p><br /></p>
            <div class="row align-items-center">
                <img src="img/cat_strategy_qualitative/5.png" width="100%">
            </div>
            <p><br /></p>
            <div class="row align-items-center">
                <img src="img/cat_strategy_qualitative/6.png" width="100%">
            </div>
            <p><br /></p>
        </div>

        <div class="section">
            <a name="bridge0shot"></a>
            <h2>Zero shot generalization on real-world scene of Bridge model</h2>
            <hr>
            <p>
                Below, we provide some qualitative examples video diffusion model trained on Bridge and tested on real-world scene without fine-tuning. (The generated videos are blurred because the original resolution is only 48x64)
            </p>
            <div class="row align-items-center">
                <div class="col justify-content-center text-center">
                    <img src="img/bridge_zshot/pick up banana/IMG_2115.jpg" width="100%">
                    <div class="overlay">
                        <div class="text"><b>Task: </b>pick up banana</div>
                    </div>
                </div>
                <div class="col justify-content-center text-center">
                    <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                        <source src="img/bridge_zshot/pick up banana/IMG_2115.mp4" type="video/mp4">
                    </video>
                    <div class="overlay">
                        <div class="text">generated video</div>
                    </div>
                </div>
                <div class="col justify-content-center text-center">
                    <img src="img/bridge_zshot/put lid on pot/IMG_2121.jpg" width="100%">
                    <div class="overlay">
                        <div class="text"><b>Task: </b>put lid on pot</div>
                    </div>
                </div>
                <div class="col justify-content-center text-center">
                    <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                        <source src="img/bridge_zshot/put lid on pot/IMG_2121.mp4" type="video/mp4">
                    </video>
                    <div class="overlay">
                        <div class="text">generated video</div>
                    </div>
                </div>
            </div>
            <p><br /></p>
            <div class="row align-items-center">
                <div class="col justify-content-center text-center">
                    <img src="img/blank.png" width="100%">
                    <div class="overlay">
                        <div class="text"> </div>
                    </div>
                </div>
                <div class="col justify-content-center text-center">
                    <img src="img/bridge_zshot/put pot in sink/IMG_2120.jpg" width="100%">
                    <div class="overlay">
                        <div class="text"><b>Task: </b>put pot in sink</div>
                    </div>
                </div>
                <div class="col justify-content-center text-center">
                    <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                        <source src="img/bridge_zshot/put pot in sink/IMG_2120.mp4" type="video/mp4">
                    </video>
                    <div class="overlay">
                        <div class="text">generated video</div>
                    </div>
                </div>
                <div class="col justify-content-center text-center">
                    <img src="img/blank.png" width="100%">
                    <div class="overlay">
                        <div class="text"> </div>
                    </div>
                </div>
            </div>
            <p><br /></p>
        </div>

        <div class="section">
            <a name="vidgenresults"></a>
            <h2>Video generation results</h2>
            <hr>
            <p>
                In this section, we provide some qualitative examples of video generation results from Meta-World, iTHOR, and Bridge models. The first two columns are first(input) and last image from GT dataset respectively. (The final frame generated by the model should
                match the second column.) From the third to the last column are the generated frames from the model. These results were generated from fully trained models.
            </p>
            <div class="overlay">
                <div class="text"><b>Meta-World</b></div>
            </div>
            <div class="row align-items-center">
                <img src="img/MW_generation.png" width="100%">
            </div>
            <p><br /></p>
            <div class="overlay">
                <div class="text"><b>iTHOR</b></div>
            </div>
            <div class="row align-items-center">
                <img src="img/iTHOR_generation.png" width="100%">
            </div>
            <p><br /></p>
            <div class="overlay">
                <div class="text"><b>Bridge</b> Since our Bridge model is trained on low-resolution, tracking objects may be harder then other environments. We used bounding-boxes to highlight the object that is being manipulated and hope this can help you when looking
                    at the results. </div>
            </div>
            <div class="row align-items-center">
                <img src="img/Bridge_generation.png" width="100%">
            </div>
            <p><br /></p>
        </div>

        <div class="section">
            <a name="DDIMresults"></a>
            <h2>DDIM results</h2>
            <hr>
            <p>
                To empirically demonstrate the possibility of significantly improving our method’s efficiency, we have conducted an additional experiment in Meta-World that generates videos using DDIM. Instead of iterative denoising 100 steps, as reported in the main
                paper, we have experimented with different numbers of denoising steps (e.g., 25, 10, 5, 3). We found that with DDIM we can generate high-fidelity videos with only 1/10 of the samplimg steps (10 steps).
            </p>
            <p>

            </p>
            <div class="overlay">
                <div class="text"><b>DDIM 25 steps: </b>We can see a little more temporal inconsistency (gripper/object disappeared/duplicated) than DDPM (100 steps) reported in last section. But the generation quality is still good in general. </div>
            </div>
            <div class="row align-items-center">
                <img src="img/DDIM_results/DDIM25.png" width="100%">
            </div>
            <p><br /></p>
            <div class="overlay">
                <div class="text"><b>DDIM 10 steps: </b> We think the generation quality is similar to DDIM 25 steps. </div>
            </div>
            <div class="row align-items-center">
                <img src="img/DDIM_results/DDIM10.png" width="100%">
            </div>
            <p><br /></p>
            <div class="overlay">
                <div class="text"><b>DDIM 5 steps: </b> We can see the temporal inconsistency happens more frequently in this setting. </div>
            </div>
            <div class="row align-items-center">
                <img src="img/DDIM_results/DDIM5.png" width="100%">
            </div>
            <p><br /></p>
            <div class="overlay">
                <div class="text"><b>DDIM 3 steps: </b> We start to see some blurred object (inconsistency within frame) </div>
            </div>
            <div class="row align-items-center">
                <img src="img/DDIM_results/DDIM3.png" width="100%">
            </div>
            <p><br /></p>
        </div>
    </div>
</body>

<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js" integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI" crossorigin="anonymous"></script>

</html>